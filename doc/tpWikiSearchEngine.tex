\documentclass[mathserif, 11pt,c]{article}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}


\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=15mm,
 right=15mm,
 top=20mm,
 }



\bibliographystyle{plain}




\usepackage{url}


\usepackage{natbib}
\bibliographystyle{abbrvnat}
 



\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{amsmath}

\newcommand{\bigslant}[2]{{\raisebox{.2em}{$#1$}\left/\raisebox{-.2em}{$#2$}\right.}}



\newtheorem{prop}{Proposition}[section] 

%opening
%\institute[MPI]{MPI \and SoedingLab}
\title{Woogle: Mini Wiki Search Engine\\
\Large Hands-on session for the Information Retrieval lecture}
\author{Clovis Galiez}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}

\setcounter{secnumdepth}{1} 

\newcommand{\normtwo}[1]{\left|\left|#1\right|\right|_2} 
\newcommand{\normmat}[1]{\left|\left|\left|#1\right|\right|\right|} 
\newcommand{\n}[1]{\lVert #1 \rVert} 
\newcommand{\myd}[2]{ 2\log{\dfrac{\n{#1}.\n{#2}}{#1\cdot #2}}}

\maketitle


We propose here to develop a mini search engine to mine documents from a wikimedia source. A skeleton of the program is provided, and we guide you in the completion of this program to achieve a search engine implementing the standard techniques of Information Retrieval.

The complete understanding of every lines of codes (especially in the technical bash scripts) is not necessary. We advise to read the provided code in hindsight to get an overview of the global process. Drawing a scheme of the data flow may be helpful.



\section{Setting up your environment - few minutes}

Get the Git repository from the following URL: \url{https://github.com/ClovisG/WikiSearchEngine} by running the following:

\texttt{git clone https://github.com/ClovisG/WikiSearchEngine.git}

You may need the follwing Python modules: \texttt{httplib2,numpy}. To install in user mode (when you do not have root access you ca use the following command:

\texttt{pip3 install --user numpy httplib2}

\section{Crawling the data - 15 min}



This section describes how the data has been collected. \textbf{No coding is asked on your side.}

We developed the script \texttt{crawl.py} that requests the Wikipedia API for list of pages in a given category. By looking in the script, you will discover a straightforward querying to \url{http://en.wikipedia.org/w/api.php} asking for \texttt{"categorymembers"} of a given category. You can choose to retrieve the pages in the subcategories up to a depth controled by the parameter \texttt{crawlingDepth}.


After checking (quickly) the code, you should be able to answer the following questions:
\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item Which Wikipedia category is crawled in this script?
	\item What does this script output?
	\item When running the script like \texttt{python3 crawl.py}, what should the file \texttt{wiki.lst} contain?
\end{enumerate}



When setting \texttt{crawlingDepth = 2}, we get about $2\,000$ pages which is enough for the purpose of this hands-on session\footnote{When setting \texttt{crawlingDepth = 4}, we got about $70\,000$ pages. The documents have already been pre-processed and the results can be found in the \url{/matieres} directory, which you can use in case time allows for testing or optimizing your final solutions.}.

\section{Downloading the data - 15 min}

We developed a simple bash script \texttt{dw.sh} that takes the file \texttt{wiki.lst} as argument and download the related Wikipedia pages. Due to some limitation on the Wikipedia side, we had to download the pages by batches.

By looking in the \texttt{dw.sh} script, can you tell:
\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item How many pages per batch is downloaded?
	\item What API of wikipedia is used to download a set of pages?
	\item By going to the API page in your browser, and reading the documentation paragraph, can you tell in what format the pages will be encoded?
\end{enumerate}


\section{Parsing the data - 2h}

\textbf{From now on, the scripts are only partial, you will have to complete them up.}


A parser of the Wikipedia documents has been developed in the \texttt{parsexml.py} that creates a \textit{tokdoc} matrix as well as a \textit{jump} matrix for the \textit{random surfer model}.




\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item From the code, how are encoded the two matrices (\textit{i.e.} what type of Python object)? What is the name of this encoding?
	\item Take a look at the database of Wikipedia documents in the \texttt{dws} folder, for example using the command \texttt{vi} or \texttt{less}. How are the links encoded in the wiki language?
	\item The regular expression for extracting the links has been removed. Propose a regular expression to detect the links in the Wikipedia format.
	\item Implement your regular expression in Python such that the first group contains the link description\footnote{Here are some examples: \begin{itemize}
		\item \texttt{"([\^{}a]+)"} will match any word having \textbf{no} \texttt{a}
		\item When parsing \texttt{Hello Bob!}, the pattern \texttt{"Hello ([\^{}a]+)!"} will extract \texttt{Bob} in the first group 
		\item Mind the necessity of \textit{escaping }characters: \texttt{"(\textbackslash [+)"} is the pattern to match any series of \texttt{[} character like \texttt{[[[[}
	\end{itemize} See \url{https://docs.python.org/3/howto/regex.html} for a description of the syntax.}.
\end{enumerate}


\section{PageRank of the documents - 2h}

The PageRank algorithm is implemented in the \texttt{pageRank.py} script.


\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item In the \textit{random surfer model}, at each iteration, random clicks are "simulated" with a given probability. Complete the code with the correct probability.
	\item What is the name of the effect we circumvent by adding \texttt{sourceVector} to the newly computed page rank vector \texttt{pageRanksNew}?
	\item Implement the formula of the convergence $\delta$.
	\item Run the PageRank program in interactive mode\footnote{Interactive mode means that Python will runthe script and give you a shell afterwards to type in any code, allowin to use the functions and variables computed in the script. Syntax: \texttt{python3 -i yourScript.py}} \texttt{python3 -i pageRank.py}, and use the Python interface to answer the following:
	\begin{itemize}
		\item How many iteration did it need to converge?
		\item What is the page rank of "Charles Darwin"?
		\item What is the page with the highest rank?
	\end{itemize}
\end{enumerate}

\section{Woogle! - 1h}


Take a look at the file \texttt{search.py} and change the \texttt{CHANGE\_ME} statements to their right formula in order to get a correct TF-IDF matrix using a \textit{sparse }representation.

If you run in interactive mode the \texttt{search.py}, get the best 15 results by the vector model for the query \textit{theory of evolution}.

\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item What type of page is selected by the vector model? By looking at the Wikipedia page, how can you explain it? What is the name of this classical \textit{cheating}?
	\item Propose and implement a way of correcting this phenomenon. Check if this correct the effect for the top 15 pages.
	\item Rank the results according to pageRank (using the \texttt{rankResults} function), and print them using the \texttt{printResults} function. Does it look nice?
	\item Take a look at vector model rankings for the query \textit{evolution of bacteria}. What is the rank of the page \textit{"Bacterial evolution"}? Rank the results by pageRank. What is the rank of the page \textit{"Bacterial evolution"}? Is it expected? How would you correct for it\footnote{We shall see this point in the next lecture :)}?
\end{enumerate}


\section{Extras}

\subsection{Effect of the source vector}
You can tune the page source in the Random Surfer Model to put more emphasis on the source vector, as well as modifying the source vector itself.
Note that handling this properly is a bit counterintuitive.


\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item Without changing anything, note down the page ranks of "DNA" and "RNA".
	\item For example, set the source vector as the pages with DNA in the title and re-compute the page rank vector. What is the new page rank of "DNA" and "RNA"? How can you explain this? How should you modify the source vector in order to increase the "DNA" page rank?
\end{enumerate}

\subsection{Effect of stemming}	

In the current implementation the words \textit{bacteria} and \textit{bacterial} are different. 
\begin{enumerate}[label=\textbf{Q\thesection.\arabic*}]
	\item List some simple grammar rules for stemming in English, and implement them in the right script. Does it improve the performance of the search? What if you use a standard library to do it?
\end{enumerate}

\subsection{Embeddings}

The current version ignore synonymy. Try out the proposed libraries in the lecture for representing words with embeddings. Devise a way of representing a document using the word embeddings and try out the effect on document querying.

\subsection{Latent semantics}

The current version ignore synonymy. Using the sparse implementation of SVD from the \textit{scipy} module, implement a simple version of the latent semantics technique. Note that this method computes the correlation between tokens, and the quantity of documents is therefore critical to have an accurate estimation of co-occurring tokens. Implement and test your method with the current dataset, and use the bigger dataset in the end to achieve better results.



%\bibliography{bib}


\end{document}